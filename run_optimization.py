"""
Main file to run optimization
Integrates all split modules
"""

import os
import sys
from pathlib import Path

# Import split modules
from load_data import load_all_data, save_data_to_pkl, load_data_from_pkl
from build_graph import build_graph_structure
from calculate_paths import calculate_all_paths, load_paths_from_pkl


def main():
    """
    Main function to run optimization
    """
    print("\n" + "="*80)
    print("GRAPH-AWARE LOGISTICS PLANNER - OPTIMIZATION RUNNER")
    print("="*80)
    
    # ============================================================
    # STEP 1: Load data (CELL 2)
    # ============================================================
    print("\n" + "="*80)
    print("STEP 1: LOAD DATA (CELL 2)")
    print("="*80)
    
    arc_file = 'data/Mekong/arcs_remapped.csv'
    node_file = 'data/Mekong/nodes_remapped_with_coords.csv'
    data_pkl = 'data/preprocessed_data.pkl'
    
    # Check if pkl file exists
    if Path(data_pkl).exists():
        print(f"\n‚úì Found pkl file: {data_pkl}")
        use_pkl = input("  Use pkl file? (y/n, default=y): ").strip().lower()
        if use_pkl != 'n':
            print("  Reading from pkl...")
            data_dict = load_data_from_pkl(data_pkl)
            
            # Unpack data
            edges_raw = data_dict['edges_raw']
            OD_pairs = data_dict['OD_pairs']
            node_names = data_dict['node_names']
            node_projects = data_dict['node_projects']
            node_type = data_dict['node_type']
            node_coords = data_dict['node_coords']
            node_capacity_passenger = data_dict['node_capacity_passenger']
            node_capacity_goods = data_dict['node_capacity_goods']
            node_capacity_pcu_levels = data_dict['node_capacity_pcu_levels']
            node_invest_levels = data_dict['node_invest_levels']
            real_nodes = data_dict['real_nodes']
            existing_hubs = data_dict['existing_hubs']
            potential_hubs = data_dict['potential_hubs']
            existing_arcs = data_dict['existing_arcs']
            potential_arcs = data_dict['potential_arcs']
            normal_nodes = data_dict['normal_nodes']
            candidate_hubs_new = data_dict['candidate_hubs_new']
            candidate_hubs_upgrade = data_dict['candidate_hubs_upgrade']
            potential_arcs_cap_0 = data_dict['potential_arcs_cap_0']
            potential_arcs_cap_up = data_dict['potential_arcs_cap_up']
            existing_arcs_cap = data_dict['existing_arcs_cap']
            real_arc_upgrade_costs = data_dict['real_arc_upgrade_costs']
        else:
            print("  Loading from CSV...")
            result = load_all_data(node_file, arc_file)
            (edges_raw, OD_pairs, node_names, node_projects, node_type, node_coords,
             node_capacity_passenger, node_capacity_goods,
             node_capacity_pcu_levels, node_invest_levels,
             real_nodes, existing_hubs, potential_hubs,
             existing_arcs, potential_arcs,
             normal_nodes, candidate_hubs_new, candidate_hubs_upgrade,
             potential_arcs_cap_0, potential_arcs_cap_up, existing_arcs_cap,
             real_arc_upgrade_costs) = result
            
            # Save to pkl
            data_dict = {
                'edges_raw': edges_raw,
                'OD_pairs': OD_pairs,
                'node_names': node_names,
                'node_projects': node_projects,
                'node_type': node_type,
                'node_coords': node_coords,
                'node_capacity_passenger': node_capacity_passenger,
                'node_capacity_goods': node_capacity_goods,
                'node_capacity_pcu_levels': node_capacity_pcu_levels,
                'node_invest_levels': node_invest_levels,
                'real_nodes': real_nodes,
                'existing_hubs': existing_hubs,
                'potential_hubs': potential_hubs,
                'existing_arcs': existing_arcs,
                'potential_arcs': potential_arcs,
                'normal_nodes': normal_nodes,
                'candidate_hubs_new': candidate_hubs_new,
                'candidate_hubs_upgrade': candidate_hubs_upgrade,
                'potential_arcs_cap_0': potential_arcs_cap_0,
                'potential_arcs_cap_up': potential_arcs_cap_up,
                'existing_arcs_cap': existing_arcs_cap,
                'real_arc_upgrade_costs': real_arc_upgrade_costs,
            }
            save_data_to_pkl(data_dict, data_pkl)
    else:
        print("  Kh√¥ng t√¨m th·∫•y file pkl, ƒëang load t·ª´ CSV...")
        result = load_all_data(node_file, arc_file)
        (edges_raw, OD_pairs, node_names, node_projects, node_type, node_coords,
         node_capacity_passenger, node_capacity_goods,
         node_capacity_pcu_levels, node_invest_levels,
         real_nodes, existing_hubs, potential_hubs,
         existing_arcs, potential_arcs,
         normal_nodes, candidate_hubs_new, candidate_hubs_upgrade,
         potential_arcs_cap_0, potential_arcs_cap_up, existing_arcs_cap,
         real_arc_upgrade_costs) = result
        
        # Save to pkl
        data_dict = {
            'edges_raw': edges_raw,
            'OD_pairs': OD_pairs,
            'node_names': node_names,
            'node_projects': node_projects,
            'node_type': node_type,
            'node_coords': node_coords,
            'node_capacity_passenger': node_capacity_passenger,
            'node_capacity_goods': node_capacity_goods,
            'node_capacity_pcu_levels': node_capacity_pcu_levels,
            'node_invest_levels': node_invest_levels,
            'real_nodes': real_nodes,
            'existing_hubs': existing_hubs,
            'potential_hubs': potential_hubs,
            'existing_arcs': existing_arcs,
            'potential_arcs': potential_arcs,
            'normal_nodes': normal_nodes,
            'candidate_hubs_new': candidate_hubs_new,
            'candidate_hubs_upgrade': candidate_hubs_upgrade,
            'potential_arcs_cap_0': potential_arcs_cap_0,
            'potential_arcs_cap_up': potential_arcs_cap_up,
            'existing_arcs_cap': existing_arcs_cap,
            'real_arc_upgrade_costs': real_arc_upgrade_costs,
        }
        save_data_to_pkl(data_dict, data_pkl)
    
    # Setup c√°c bi·∫øn c·∫ßn thi·∫øt
    T = [1, 2]
    T_len = len(T)
    N = real_nodes
    
    # T√¨m t·∫•t c·∫£ c√°c hub t·ª´ d·ªØ li·ªáu
    H = existing_hubs + potential_hubs
    H_tilde = potential_hubs
    H0 = existing_hubs
    new_hubs = candidate_hubs_new
    
    print(f"  ‚Ä¢ T·∫•t c·∫£ hubs t·ª´ d·ªØ li·ªáu: {H}")
    print(f"  ‚Ä¢ Real nodes: {len(N)} nodes")
    
    # ============================================================
    # STEP 2: Build graph (CELL 3)
    # ============================================================
    print("\n" + "="*80)
    print("STEP 2: BUILD GRAPH (CELL 3)")
    print("="*80)
    
    graph_data = build_graph_structure(edges_raw, H, N, OD_pairs)
    
    G_exp = graph_data['G_exp']
    A = graph_data['A']
    A_tilde = graph_data['A_tilde']
    A0 = graph_data['A0']
    N_virtual = graph_data['N_virtual']
    all_nodes = graph_data['all_nodes']
    OD_pairs = graph_data['OD_pairs']  # Updated v·ªõi format m·ªõi
    
    # ============================================================
    # STEP 3: Calculate paths (CELL 4 + CELL 5)
    # ============================================================
    print("\n" + "="*80)
    print("STEP 3: CALCULATE PATHS (CELL 4 + CELL 5)")
    print("="*80)
    
    paths_pkl = 'data/paths_data.pkl'
    
    # Check if pkl file exists
    if Path(paths_pkl).exists():
        print(f"\n‚úì Found pkl file: {paths_pkl}")
        use_pkl = input("  Use pkl file? (y/n, default=y): ").strip().lower()
        if use_pkl != 'n':
            print("  Reading paths t·ª´ pkl...")
            paths_data = load_paths_from_pkl(paths_pkl)
            paths = paths_data['paths']
            Lmin_dict = paths_data['Lmin_dict']
        else:
            print("  ƒêang t√≠nh to√°n paths...")
            paths, Lmin_dict = calculate_all_paths(
                G_exp, OD_pairs, H, node_names, node_projects, edges_raw,
                EPSILON=0.5, MAX_PATHS_PER_OD=5000, save_pkl=True
            )
    else:
        print("  Kh√¥ng t√¨m th·∫•y file pkl, ƒëang t√≠nh to√°n paths...")
        paths, Lmin_dict = calculate_all_paths(
            G_exp, OD_pairs, H, node_names, node_projects, edges_raw,
            EPSILON=0.5, MAX_PATHS_PER_OD=5000, save_pkl=True
        )
    
    # ============================================================
    # STEP 4: Setup costs v√† build model
    # ============================================================
    print("\n" + "="*80)
    print("STEP 4: SETUP COSTS & BUILD MODEL")
    print("="*80)
    print("\n‚ö†Ô∏è  Ph·∫ßn n√†y c·∫ßn import t·ª´ model_gurobi.py g·ªëc")
    print("   Ho·∫∑c t√°ch th√†nh c√°c module ri√™ng: setup_costs.py, build_model.py")
    print("   Hi·ªán t·∫°i b·∫°n c√≥ th·ªÉ ch·∫°y model_gurobi.py tr·ª±c ti·∫øp sau khi ƒë√£ c√≥ pkl")
    
    # ============================================================
    # Summary
    # ============================================================
    print("\n" + "="*80)
    print("SUMMARY")
    print("="*80)
    print(f"‚úì Loaded data: {len(edges_raw)} arcs, {len(node_names)} nodes")
    print(f"‚úì ƒê√£ build graph: {len(A)} arcs, {len(N_virtual)} virtual nodes")
    print(f"‚úì ƒê√£ t√≠nh paths: {sum(len(p) for p in paths.values())} paths")
    print(f"\nüìÅ Files pkl ƒë√£ t·∫°o:")
    print(f"  - {data_pkl}")
    print(f"  - {paths_pkl}")
    print(f"\nüí° ƒê·ªÉ ch·∫°y optimization, b·∫°n c√≥ th·ªÉ:")
    print(f"  1. S·ª≠ d·ª•ng model_gurobi.py v·ªõi d·ªØ li·ªáu ƒë√£ load")
    print(f"  2. Ho·∫∑c t√°ch th√™m c√°c module: setup_costs.py, build_model.py, solve_and_export.py")


if __name__ == "__main__":
    main()
